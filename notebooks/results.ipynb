{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c396c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2cc58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== in vs far-ood ===\n",
      "score_type  AUROC\n",
      "   mean_sp  0.723\n",
      "    min_sp  0.721\n",
      "\n",
      "=== in vs far-ood + near-ood ===\n",
      "score_type  AUROC\n",
      "   mean_sp  0.711\n",
      "    min_sp  0.701\n",
      "\n",
      "=== in vs near-ood ===\n",
      "score_type  AUROC\n",
      "   mean_sp  0.675\n",
      "    min_sp  0.644\n",
      "\n",
      "=== in vs far-ood ===\n",
      "score_type  PRAUC\n",
      "    min_sp  0.879\n",
      "   mean_sp  0.876\n",
      "\n",
      "=== in vs far-ood + near-ood ===\n",
      "score_type  PRAUC\n",
      "   mean_sp  0.900\n",
      "    min_sp  0.898\n",
      "\n",
      "=== in vs near-ood ===\n",
      "score_type  PRAUC\n",
      "   mean_sp  0.673\n",
      "    min_sp  0.633\n"
     ]
    }
   ],
   "source": [
    "def make_split(test_df: pd.DataFrame, pos_classes: list[str], score_type: str) -> tuple:\n",
    "    mask = test_df[\"labels\"].isin([\"in\", *pos_classes])\n",
    "    y_true = test_df.loc[mask, \"labels\"].isin(pos_classes)\n",
    "    y_score = test_df.loc[mask, score_type]\n",
    "    return y_true, y_score\n",
    "\n",
    "\n",
    "def read_baselines(base: Path, suffix: str = \"baselines.jsonl\"):\n",
    "    test_df = pd.read_json(base / suffix, lines=True)\n",
    "    results = []\n",
    "\n",
    "    for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "        split_name = \"in vs \" + \" + \".join(split)\n",
    "        for score_type in (\"mean_sp\", \"min_sp\"):\n",
    "            y_true, y_score = make_split(test_df, split, score_type)\n",
    "            auroc_val = roc_auc_score(y_true, y_score)\n",
    "            prauc_val = average_precision_score(y_true, y_score)\n",
    "            results.append({\n",
    "                \"split\": split_name,\n",
    "                \"score_type\": score_type,\n",
    "                \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def read_uq_sequence(base: Path):\n",
    "    results = []\n",
    "    for model in (\"mcdropout\", \"flipout\"):\n",
    "        for metric in (\"f1\", \"bertscore\"):\n",
    "            test_df = pd.read_json(base / f\"{model}-large-{metric}.jsonl\", lines=True)\n",
    "            for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "                split_name = \"in vs \" + \" + \".join(split)\n",
    "                y_true, y_score = make_split(test_df, split, metric)\n",
    "\n",
    "                auroc_val = roc_auc_score(y_true, y_score)\n",
    "                prauc_val = average_precision_score(y_true, y_score)\n",
    "                results.append({\n",
    "                    \"split\": split_name,\n",
    "                    \"score_type\": f\"{model}-{metric}\",\n",
    "                    \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                    \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def read_uq_token(base: Path):\n",
    "    results = []\n",
    "    for model in (\"mcdropout\",):\n",
    "        test_df = pd.read_json(base / f\"{model}-large-token.jsonl\", lines=True)\n",
    "        for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "            split_name = \"in vs \" + \" + \".join(split)\n",
    "            for score_type in (\"mean_mi\", \"max_mi\"):\n",
    "                y_true, y_score = make_split(test_df, split, score_type)\n",
    "\n",
    "                auroc_val = roc_auc_score(y_true, y_score)\n",
    "                prauc_val = average_precision_score(y_true, y_score)\n",
    "                results.append({\n",
    "                    \"split\": split_name,\n",
    "                    \"score_type\": f\"{model}-{score_type}\",\n",
    "                    \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                    \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "                })\n",
    "\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base = Path(\"../results\")\n",
    "    dataset = \"webquestions\"\n",
    "    baseline_df = read_baselines(base / dataset)\n",
    "    combined_df = baseline_df\n",
    "    # uq_sequence_df = read_uq_sequence(base / dataset)\n",
    "    # uq_token_df = read_uq_token(base / dataset)\n",
    "\n",
    "    # combined_df = pd.concat([baseline_df, uq_sequence_df, uq_token_df]).reset_index(drop=True)\n",
    "    combined_df[\"AUROC\"] = combined_df[\"AUROC\"].astype(float)           # for sorting safely\n",
    "\n",
    "    for split, group in combined_df.groupby(\"split\"):\n",
    "        print(f\"\\n=== {split} ===\")\n",
    "        group_sorted = group.sort_values(\"AUROC\", ascending=False)\n",
    "        print(group_sorted[[\"score_type\", \"AUROC\"]].to_string(index=False))\n",
    "\n",
    "    combined_df[\"PRAUC\"] = combined_df[\"PRAUC\"].astype(float)           # for sorting safely\n",
    "\n",
    "    for split, group in combined_df.groupby(\"split\"):\n",
    "        print(f\"\\n=== {split} ===\")\n",
    "        group_sorted = group.sort_values(\"PRAUC\", ascending=False)\n",
    "        print(group_sorted[[\"score_type\", \"PRAUC\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143e801",
   "metadata": {},
   "source": [
    "Removed:\n",
    "- ROC curves\n",
    "- EM scores and performance (#correct) per split. using most common answer as hard prediction. including answer normalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
