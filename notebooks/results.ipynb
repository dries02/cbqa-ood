{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c396c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2cc58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flipout-f1 does not exist\n",
      "flipout-bertscore does not exist\n",
      "\n",
      "=== in vs far-ood ===\n",
      "         score_type  AUROC\n",
      "  mcdropout-mean_mi  0.815\n",
      "   mcdropout-max_mi  0.803\n",
      "mcdropout-bertscore  0.773\n",
      "       mcdropout-f1  0.771\n",
      "            mean_sp  0.537\n",
      "             min_sp  0.530\n",
      "\n",
      "=== in vs far-ood + near-ood ===\n",
      "         score_type  AUROC\n",
      "  mcdropout-mean_mi  0.802\n",
      "   mcdropout-max_mi  0.786\n",
      "       mcdropout-f1  0.761\n",
      "mcdropout-bertscore  0.752\n",
      "            mean_sp  0.549\n",
      "             min_sp  0.532\n",
      "\n",
      "=== in vs near-ood ===\n",
      "         score_type  AUROC\n",
      "  mcdropout-mean_mi  0.751\n",
      "       mcdropout-f1  0.723\n",
      "   mcdropout-max_mi  0.719\n",
      "mcdropout-bertscore  0.662\n",
      "            mean_sp  0.600\n",
      "             min_sp  0.543\n",
      "\n",
      "=== in vs far-ood ===\n",
      "         score_type  PRAUC\n",
      "  mcdropout-mean_mi  0.937\n",
      "   mcdropout-max_mi  0.935\n",
      "mcdropout-bertscore  0.920\n",
      "       mcdropout-f1  0.919\n",
      "             min_sp  0.825\n",
      "            mean_sp  0.817\n",
      "\n",
      "=== in vs far-ood + near-ood ===\n",
      "         score_type  PRAUC\n",
      "  mcdropout-mean_mi  0.944\n",
      "   mcdropout-max_mi  0.941\n",
      "       mcdropout-f1  0.931\n",
      "mcdropout-bertscore  0.929\n",
      "             min_sp  0.855\n",
      "            mean_sp  0.854\n",
      "\n",
      "=== in vs near-ood ===\n",
      "         score_type  PRAUC\n",
      "  mcdropout-mean_mi  0.705\n",
      "   mcdropout-max_mi  0.689\n",
      "       mcdropout-f1  0.685\n",
      "mcdropout-bertscore  0.649\n",
      "            mean_sp  0.596\n",
      "             min_sp  0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342286/3683069329.py:33: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  test_df = pd.read_json(base / f\"{model}-large-{metric}.jsonl\", lines=True)\n"
     ]
    }
   ],
   "source": [
    "def make_split(test_df: pd.DataFrame, pos_classes: list[str], score_type: str) -> tuple:\n",
    "    mask = test_df[\"labels\"].isin([\"in\", *pos_classes])\n",
    "    y_true = test_df.loc[mask, \"labels\"].isin(pos_classes)\n",
    "    y_score = test_df.loc[mask, score_type]\n",
    "    return y_true, y_score\n",
    "\n",
    "\n",
    "def read_baselines(base: Path, suffix: str = \"baselines.jsonl\"):\n",
    "    test_df = pd.read_json(base / suffix, lines=True)\n",
    "    results = []\n",
    "\n",
    "    for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "        split_name = \"in vs \" + \" + \".join(split)\n",
    "        for score_type in (\"mean_sp\", \"min_sp\"):\n",
    "            y_true, y_score = make_split(test_df, split, score_type)\n",
    "            auroc_val = roc_auc_score(y_true, y_score)\n",
    "            prauc_val = average_precision_score(y_true, y_score)\n",
    "            results.append({\n",
    "                \"split\": split_name,\n",
    "                \"score_type\": score_type,\n",
    "                \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def read_uq_sequence(base: Path):\n",
    "    results = []\n",
    "    for model in (\"mcdropout\", \"flipout\"):\n",
    "        for metric in (\"f1\", \"bertscore\"):\n",
    "            try:\n",
    "                test_df = pd.read_json(base / f\"{model}-large-{metric}.jsonl\", lines=True)\n",
    "                for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "                    split_name = \"in vs \" + \" + \".join(split)\n",
    "                    y_true, y_score = make_split(test_df, split, metric)\n",
    "\n",
    "                    auroc_val = roc_auc_score(y_true, y_score)\n",
    "                    prauc_val = average_precision_score(y_true, y_score)\n",
    "                    results.append({\n",
    "                        \"split\": split_name,\n",
    "                        \"score_type\": f\"{model}-{metric}\",\n",
    "                        \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                        \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "                    })\n",
    "            except ValueError:\n",
    "                print(f\"{model}-{metric} does not exist\")\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def read_uq_token(base: Path):\n",
    "    results = []\n",
    "    for model in (\"mcdropout\",):\n",
    "        try:\n",
    "            test_df = pd.read_json(base / f\"{model}-large-token.jsonl\", lines=True)\n",
    "            for split in ([\"far-ood\"], [\"near-ood\"], [\"far-ood\", \"near-ood\"]):\n",
    "                split_name = \"in vs \" + \" + \".join(split)\n",
    "                for score_type in (\"mean_mi\", \"max_mi\"):\n",
    "                    y_true, y_score = make_split(test_df, split, score_type)\n",
    "\n",
    "                    auroc_val = roc_auc_score(y_true, y_score)\n",
    "                    prauc_val = average_precision_score(y_true, y_score)\n",
    "                    results.append({\n",
    "                        \"split\": split_name,\n",
    "                        \"score_type\": f\"{model}-{score_type}\",\n",
    "                        \"AUROC\": f\"{auroc_val:.3f}\",\n",
    "                        \"PRAUC\": f\"{prauc_val:.3f}\",\n",
    "                    })\n",
    "        except ValueError:\n",
    "            print(f\"{model}-large-token.jsonl does not exist\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base = Path(\"../results\")\n",
    "    dataset = \"nq\"\n",
    "    baseline_df = read_baselines(base / dataset)\n",
    "    uq_sequence_df = read_uq_sequence(base / dataset)\n",
    "    uq_token_df = read_uq_token(base / dataset)\n",
    "\n",
    "    combined_df = pd.concat([baseline_df, uq_sequence_df, uq_token_df]).reset_index(drop=True)\n",
    "    combined_df[\"AUROC\"] = combined_df[\"AUROC\"].astype(float)           # for sorting safely\n",
    "\n",
    "    for split, group in combined_df.groupby(\"split\"):\n",
    "        print(f\"\\n=== {split} ===\")\n",
    "        group_sorted = group.sort_values(\"AUROC\", ascending=False)\n",
    "        print(group_sorted[[\"score_type\", \"AUROC\"]].to_string(index=False))\n",
    "\n",
    "    combined_df[\"PRAUC\"] = combined_df[\"PRAUC\"].astype(float)           # for sorting safely\n",
    "\n",
    "    for split, group in combined_df.groupby(\"split\"):\n",
    "        print(f\"\\n=== {split} ===\")\n",
    "        group_sorted = group.sort_values(\"PRAUC\", ascending=False)\n",
    "        print(group_sorted[[\"score_type\", \"PRAUC\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143e801",
   "metadata": {},
   "source": [
    "Removed:\n",
    "- ROC curves\n",
    "- EM scores and performance (#correct) per split. using most common answer as hard prediction. including answer normalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
